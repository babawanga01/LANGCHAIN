{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0a6944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d358327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader as CommunityTextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83302546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24597a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89ce982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s).\n",
      "mode now. No, it's just a PowerPoint slides.\n",
      "0:08\n",
      "Not yet. I think there's a separate screen. So if you try to share again,\n",
      "0:12\n",
      "Eric, there will be a separate screen. That's more\n",
      "0:15\n",
      "That's what I feared. Yeah. Okay. Um so I will just keep\n",
      "0:23\n",
      "sharing the the present not not the presenter presenter view but just the\n",
      "0:28\n",
      "regular one. Um so okay um I put together this to kind of go over um the\n",
      "0:35\n",
      "larger plan uh when I talked with uh JP about the best project and I think\n",
      "0:40\n",
      "we had all talked about that um we had settled on the\n",
      "0:46\n",
      "um SDSQC AI for the project to work with your\n",
      "0:52\n",
      "team o\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# load Meeting trasncript.txt into `data` using a langchain_community loader if available,\n",
    "# otherwise fall back to LangChain's TextLoader or a manual Document wrapper.\n",
    "\n",
    "try:\n",
    "    # try community loader\n",
    "    loader = CommunityTextLoader(\"Meeting transcript.txt\")\n",
    "    data = loader.load()\n",
    "except Exception:\n",
    "    try:\n",
    "        # fall back to built-in LangChain loader\n",
    "        loader = TextLoader(\"Meeting transcript.txt\")\n",
    "        data = loader.load()\n",
    "    except Exception:\n",
    "        # final fallback: read file manually and wrap as a LangChain Document\n",
    "        with open(\"Meeting transcript.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        data = [Document(page_content=content, metadata={\"source\": \"Meeting transcript.txt\"})]\n",
    "\n",
    "# quick sanity check\n",
    "print(f\"Loaded {len(data)} document(s).\")\n",
    "print(data[0].page_content[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dec1a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 document(s) from datap2.pdf.\n",
      "UNITED STATES DISTRICT COURT \n",
      "SOUTHERN DISTRICT OF FLORIDA \n",
      "MIAMI DIVISION \n",
      "CAS\n",
      "E NO.: 23-cv-22148-GAYLES/LOUIS \n",
      "ROSH CHODESH II LIMITED \n",
      "PARTNERSHIP, et al., \n",
      "P\n",
      "laintiffs, \n",
      "v.\n",
      " \n",
      "JA\n",
      "N S. WIMPFHEIMER, et al., \n",
      "D\n",
      "efendants.   \n",
      "_____________________________/            \n",
      "O\n",
      "RDER \n",
      "T\n",
      "HIS CAUSE comes  before the Court on the Motion to Dismiss the Second Amended \n",
      "Complaint filed by Defendants Madison Gold LLC, Jan S. Wimpfheimer, and Schwell \n",
      "Wimpfheimer & Associates, LLP, [ECF No. 72] (“Madison Gold’s Motion to Dismiss”) and the \n",
      "Motion to Dismiss the Second Amended Complaint filed by Defendants East Hudson Capital, LLC \n",
      "and White Road Capital LLC [ECF No. 71] (“East Hudson’s Motion to Dismiss”). This case was \n",
      "referred to Magistrate Judge Lauren F. Louis  for a ruling on all pretrial non- dispositive matters \n",
      "and a report and recommendation on all dispositive matters, pursuant to 28 U.S.C. § 636(b)(1)(B). \n",
      "[ECF No. 48]. On February 4, 2025, she  issued her report recommending that  Madison Go\n"
     ]
    }
   ],
   "source": [
    "# Load datap.pdf into `data_pdf`.\n",
    "pdf_path = \"datap2.pdf\"\n",
    "data_pdf = None\n",
    "\n",
    "# Try community loader candidates (if present in langchain_community.document_loaders)\n",
    "for candidate in (\"PyPDFLoader\", \"PDFMinerLoader\", \"UnstructuredPDFLoader\", \"PDFLoader\"):\n",
    "    loader_cls = getattr(langchain_community.document_loaders, candidate, None)\n",
    "    if loader_cls:\n",
    "        try:\n",
    "            data_pdf = loader_cls(pdf_path).load()\n",
    "            break\n",
    "        except Exception:\n",
    "            data_pdf = None\n",
    "\n",
    "# Fallback to LangChain's loaders\n",
    "if not data_pdf:\n",
    "    try:\n",
    "        data_pdf = PyPDFLoader(pdf_path).load()\n",
    "    except Exception:\n",
    "        try:\n",
    "            data_pdf = UnstructuredPDFLoader(pdf_path).load()\n",
    "        except Exception:\n",
    "            # Final fallback: extract text with PyPDF2\n",
    "            try:\n",
    "                with open(pdf_path, \"rb\") as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    texts = [p.extract_text() or \"\" for p in reader.pages]\n",
    "                content = \"\\n\".join(texts)\n",
    "                data_pdf = [Document(page_content=content, metadata={\"source\": pdf_path})]\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Could not load PDF {pdf_path}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(data_pdf)} document(s) from {pdf_path}.\")\n",
    "print(data_pdf[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f131d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23 document(s) from datap.pdf.\n",
      "Case 2:24-cv-08076-CAS-SSC     Document 28     Filed 02/24/25     Page 2 of 23   Page ID\n",
      "#:791\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(data_pdf)} document(s) from {pdf_path}.\")\n",
    "print(data_pdf[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8a4bfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNITED STATES DISTRICT COURT \n",
      "SOUTHERN DISTRICT OF FLORIDA \n",
      "MIAMI DIVISION \n",
      "CAS\n",
      "E NO.: 23-cv-22148-GAYLES/LOUIS \n",
      "ROSH CHODESH II LIMITED \n",
      "PARTNERSHIP, et al., \n",
      "P\n",
      "laintiffs, \n",
      "v.\n",
      " \n",
      "JA\n",
      "N S. WIMPFHEIMER, et al., \n",
      "D\n",
      "efendants.   \n",
      "_____________________________/            \n",
      "O\n",
      "RDER \n",
      "T\n",
      "HIS CAUSE comes  before the Court on the Motion to Dismiss the Second Amended \n",
      "Complaint filed by Defendants Madison Gold LLC, Jan S. Wimpfheimer, and Schwell \n",
      "Wimpfheimer & Associates, LLP, [ECF No. 72] (“Madison Gold’s Motion to Dismiss”) and the \n",
      "Motion to Dismiss the Second Amended Complaint filed by Defendants East Hudson Capital, LLC \n",
      "and White Road Capital LLC [ECF No. 71] (“East Hudson’s Motion to Dismiss”). This case was \n",
      "referred to Magistrate Judge Lauren F. Louis  for a ruling on all pretrial non- dispositive matters \n",
      "and a report and recommendation on all dispositive matters, pursuant to 28 U.S.C. § 636(b)(1)(B). \n",
      "[ECF No. 48]. On February 4, 2025, she  issued her report recommending that  Madison Gold’s \n",
      "Motion to Dismiss be granted, and that East Hudson’s Motion to Dismiss be denied as moot [ECF \n",
      "No. 93]. No objections were filed to the Report. \n",
      "A district court may accept, reject, or modify a magistrate judge’s report and \n",
      "recommendation. 28 U.S.C. § 636(b)(1). Those portions of the report and recommendation to which \n",
      "objection is made are accorded de novo review, if those objections “pinpoint the specific findings \n",
      "Case 1:23-cv-22148-DPG   Document 94   Entered on FLSD Docket 02/26/2025   Page 1 of 2\n"
     ]
    }
   ],
   "source": [
    "print(data_pdf[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1588f5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s) from https://arxiv.org/abs/1706.03762\n",
      "\n",
      "\n",
      " [1706.03762] Attention Is All You Need\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Happy Open Access Week from arXiv!\n",
      "YOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n",
      "\n",
      "\n",
      "Donate!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n",
      "Donate\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " > cs > arXiv:1706.03762\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help | Advanced Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All fields\n",
      "Title\n",
      "Author\n",
      "Abstract\n",
      "Comments\n",
      "Journal reference\n",
      "ACM classification\n",
      "MSC classification\n",
      "Report number\n",
      "arXiv identifier\n",
      "DOI\n",
      "ORCID\n",
      "arXiv author ID\n",
      "Help pages\n",
      "Full text\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "open search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GO\n",
      "\n",
      "\n",
      "\n",
      "open navigation menu\n",
      "\n",
      "\n",
      "quick links\n",
      "\n",
      "Login\n",
      "Help Pages\n",
      "About\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science > Computation and Language\n",
      "\n",
      "\n",
      "arXiv:1706.03762 (cs)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  [Submitted on 12 Jun 2017 (v1), last revised 2 Aug 2023 (this version, v7)]\n",
      "Title:Attention Is All You Need\n",
      "Authors:Ashish Vaswani, Noam Shazeer, Ni\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader as LCWebBaseLoader\n",
    "\n",
    "\n",
    "# Load https://ecourts.gov.in/ecourts_home/ into `data_web`, preferring community loaders, then langchain loaders,\n",
    "# then falling back to requests + BeautifulSoup if necessary.\n",
    "\n",
    "url = \"https://arxiv.org/abs/1706.03762\"\n",
    "data_web = None\n",
    "\n",
    "# Try community loader candidates (if present in langchain_community.document_loaders)\n",
    "for candidate in (\"PlaywrightURLLoader\", \"SeleniumURLLoader\", \"WebBaseLoader\", \"UnstructuredURLLoader\", \"BeautifulSoupURLLoader\"):\n",
    "    loader_cls = getattr(langchain_community.document_loaders, candidate, None)\n",
    "    if loader_cls:\n",
    "        try:\n",
    "            # some loaders accept a single URL (string) or a list; try both forms gracefully\n",
    "            try:\n",
    "                data_web = loader_cls([url]).load()\n",
    "            except Exception:\n",
    "                data_web = loader_cls(url).load()\n",
    "            break\n",
    "        except Exception:\n",
    "            data_web = None\n",
    "\n",
    "# Fallback to LangChain's URL loader(s)\n",
    "if not data_web:\n",
    "    try:\n",
    "        # Try built-in LangChain WebBaseLoader if available\n",
    "        data_web = LCWebBaseLoader(url).load()\n",
    "    except Exception:\n",
    "        data_web = None\n",
    "\n",
    "# Final fallback: requests + BeautifulSoup\n",
    "if not data_web:\n",
    "\n",
    "    resp = requests.get(url, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    soup = bs4.BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for s in soup([\"script\", \"style\", \"noscript\", \"iframe\", \"header\", \"footer\", \"nav\"]):\n",
    "        s.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    # collapse multiple blank lines and trim\n",
    "    text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text).strip()\n",
    "    data_web = [Document(page_content=text, metadata={\"source\": url})]\n",
    "\n",
    "print(f\"Loaded {len(data_web)} document(s) from {url}\")\n",
    "print(data_web[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61ff9837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted abstract text (first 500 chars):\n",
      "\n",
      "\n",
      " [1706.03762] Attention Is All You Need\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Happy Open Access Week from arXiv!\n",
      "YOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n",
      "\n",
      "\n",
      "Donate!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n",
      "Donate\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " > cs > arXiv:1706.03762\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help | Advanced Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All fields\n",
      "Title\n",
      "Author\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract only the abstract section from whatever was loaded into `data_web`.\n",
    "# This will work whether `data_web[0].page_content` is raw HTML or plain text.\n",
    "if not data_web:\n",
    "    raise RuntimeError(\"data_web is empty; nothing to extract from.\")\n",
    "\n",
    "content = data_web[0].page_content or \"\"\n",
    "\n",
    "# Try HTML parsing first (blockquotes with class `abstract mathjax` on arXiv)\n",
    "abstract_text = \"\"\n",
    "if \"<\" in content and \">\" in content:\n",
    "    soup = bs4.BeautifulSoup(content, \"html.parser\")\n",
    "    block = soup.find(\"blockquote\", class_=\"abstract mathjax\") or soup.find(class_=\"abstract mathjax\")\n",
    "    if block:\n",
    "        abstract_text = block.get_text(separator=\"\\n\").strip()\n",
    "        abstract_text = re.sub(r\"^\\s*Abstract[:\\s]*\", \"\", abstract_text, flags=re.I)\n",
    "\n",
    "# Fallback to regex on plain text output\n",
    "if not abstract_text:\n",
    "    # Try to capture content after 'Abstract' heading up to a blank line or end of document\n",
    "    m = re.search(r\"(?is)abstract[:\\s]*\\n?(.*?)(?:\\n\\s*\\n|\\Z)\", content)\n",
    "    if m:\n",
    "        abstract_text = m.group(1).strip()\n",
    "    else:\n",
    "        # Last fallback: attempt to find a short paragraph that looks like an abstract\n",
    "        # (first paragraph after the title/metadata)\n",
    "        paragraphs = [p.strip() for p in re.split(r\"\\n{2,}\", content) if p.strip()]\n",
    "        if paragraphs:\n",
    "            abstract_text = paragraphs[0]\n",
    "\n",
    "# Normalize whitespace\n",
    "abstract_text = re.sub(r\"\\s+\", \" \", abstract_text).strip()\n",
    "\n",
    "\n",
    "print(\"Extracted abstract text (first 500 chars):\")\n",
    "print(data_web[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b7ec4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s) from https://medium.com/@joerosborne/intro-to-web-scraping-build-your-first-scraper-in-5-minutes-1c36b5c4b110\n",
      "Just a moment...Enable JavaScript and cookies to continue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(data_web)} document(s) from {url}\")\n",
    "print(data_web[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34950775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57204927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s) from https://en.wikipedia.org/wiki/Infosys\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Infosys - Wikipedia\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jump to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\t\tNavigation\n",
      "\t\n",
      "\n",
      "\n",
      "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tContribute\n",
      "\t\n",
      "\n",
      "\n",
      "HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Appearance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Donate\n",
      "\n",
      "Create account\n",
      "\n",
      "Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Personal tools\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Donate Create account Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Top)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "History\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "Services and products\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "Acquisitions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      "Listing and shareholding pattern\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "Operations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle Operations subsection\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.1\n",
      "Geographical presence\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.2\n",
      "Training centre in Mysore\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.3\n",
      "Employees\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.4\n",
      "CEOs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "Controversies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle Controversies subsection\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6.1\n",
      "Settlement of visa and tax fraud cases in the US\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6.2\n",
      "Malfunctioning government portals\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Wikipedia page for company \"Infosys\" into `data_wiki`, preferring community loaders,\n",
    "# then LCWebBaseLoader, and finally falling back to requests + BeautifulSoup.\n",
    "\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/Infosys\"\n",
    "data_wiki = None\n",
    "\n",
    "# Try community loader candidates (if present in langchain_community.document_loaders)\n",
    "for candidate in (\"PlaywrightURLLoader\", \"SeleniumURLLoader\", \"WebBaseLoader\", \"UnstructuredURLLoader\", \"BeautifulSoupURLLoader\"):\n",
    "    loader_cls_candidate = getattr(langchain_community.document_loaders, candidate, None)\n",
    "    if loader_cls_candidate:\n",
    "        try:\n",
    "            try:\n",
    "                data_wiki = loader_cls_candidate([wiki_url]).load()\n",
    "            except Exception:\n",
    "                data_wiki = loader_cls_candidate(wiki_url).load()\n",
    "            break\n",
    "        except Exception:\n",
    "            data_wiki = None\n",
    "\n",
    "# Fallback to LCWebBaseLoader if community loaders weren't successful\n",
    "if not data_wiki:\n",
    "    try:\n",
    "        data_wiki = LCWebBaseLoader(wiki_url).load()\n",
    "    except Exception:\n",
    "        data_wiki = None\n",
    "\n",
    "# Final fallback: requests + BeautifulSoup\n",
    "if not data_wiki:\n",
    "    resp = requests.get(wiki_url, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    soup_local = bs4.BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for s in soup_local([\"script\", \"style\", \"noscript\", \"iframe\", \"header\", \"footer\", \"nav\"]):\n",
    "        s.decompose()\n",
    "\n",
    "    text = soup_local.get_text(separator=\"\\n\")\n",
    "    text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text).strip()\n",
    "    data_wiki = [Document(page_content=text, metadata={\"source\": wiki_url})]\n",
    "\n",
    "# Sanity checks / output\n",
    "print(f\"Loaded {len(data_wiki)} document(s) from {wiki_url}\")\n",
    "print(data_wiki[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 document(s) using WikipediaLoader.\n",
      "Infosys Limited is an Indian multinational technology company that offers information technology, business consulting, and outsourcing services. Founded in 1981 by seven engineers, the company is headquartered in Bengaluru and considered one of the Big Six Indian IT companies.\n",
      "Infosys has also attracted controversies due to allegations of visa and tax fraud in the United States and for creating malfunctioning government websites.\n",
      "\n",
      "\n",
      "== History ==\n",
      "Infosys was founded by N. R. Narayana Murthy, Nandan Nilekani, Kris Gopalakrishnan, S. D. Shibulal, K. Dinesh, N. S. Raghavan, and Ashok Arora, with an initial capital of $250. It was incorporated as Infosys Consultants Private Limited in Pune on 2 July 1981, before relocating to Bangalore in 1983. Arora left the company in 1989 and sold his shares to the other co-founders.\n",
      "In the 1980s, Infosys briefly made hardware products like electronic telex machines and keyboard concentrators. Its core business of offshore custom software development wit\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "# Load Wikipedia page for \"Infosys\" using WikipediaLoader\n",
    "wiki_loader = WikipediaLoader(query=\"Infosys\")\n",
    "data_wiki_lc = wiki_loader.load()\n",
    "\n",
    "print(f\"Loaded {len(data_wiki_lc)} document(s) using WikipediaLoader.\")\n",
    "print(data_wiki_lc[0].page_content[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00f60aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The south Indian city of Chennai is fast emerging as a destination for information technology outsourcing and has seen a growing number of IT parks being built here. Most of the upcoming complexes are being built along the IT Corridor and the southern suburb.\n",
      "\n",
      "\n",
      "== List ==\n",
      "\n",
      "\n",
      "== See also ==\n",
      "List of tech parks in Kolkata\n",
      "Software industry in Chennai\n",
      "\n",
      "\n",
      "== References ==\n"
     ]
    }
   ],
   "source": [
    "print(data_wiki_lc[20].page_content[:4000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
